import torch
from PIL import Image
from transformers import BertTokenizer, ViTFeatureExtractor
import gradio as gr
import json
import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

from config.Config_base import Config_base
from model.MHKE import MHKE, MHKE_CLIP

# --- Part 1: æ¨¡å‹å’Œé¢„å¤„ç†å™¨åŠ è½½ (åç«¯æ ¸å¿ƒé€»è¾‘) ---
print("æ­£åœ¨åŠ è½½é…ç½®å’Œæ·±åº¦èåˆæ¨¡å‹ï¼Œè¯·ç¨å€™...")

model_to_load = "MHKE"
task_name = "task_2"
config = Config_base(model_to_load, task_name)

tokenizer = BertTokenizer.from_pretrained(config.roberta_path)
image_extractor = ViTFeatureExtractor.from_pretrained(config.vit_path)
if model_to_load == "clip":
    from transformers import CLIPProcessor

    processor = CLIPProcessor.from_pretrained(config.clip_path)
    tokenizer = processor
    image_extractor = processor

print(f"æ­£åœ¨å®ä¾‹åŒ–æ”¹è¿›åçš„æ·±åº¦èåˆæ¨¡å‹: {model_to_load}")
if model_to_load == "MHKE":
    model = MHKE(config).to(config.device)
    checkpoint_path = '{}/ckp-MHKE_B-32_E-10_Lr-1e-05_w-0.5_task_2_add_Fusion-BEST.tar'.format(config.checkpoint_path)
elif model_to_load == "clip":
    model = MHKE_CLIP(config).to(config.device)
    checkpoint_path = '{}/ckp-clip_B-32_E-10_Lr-1e-05_w-0.5_task_2_add_Fusion-BEST.tar'.format(config.checkpoint_path)
else:
    raise ValueError("æœªçŸ¥çš„æ¨¡å‹åç§°")

try:
    checkpoint = torch.load(checkpoint_path, map_location=config.device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"æˆåŠŸåŠ è½½æ·±åº¦èåˆæ¨¡å‹æƒé‡: {checkpoint_path}")
except FileNotFoundError:
    print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {checkpoint_path}ã€‚è¯·ç¡®è®¤æ‚¨çš„æœ€ä½³æ¨¡å‹æ–‡ä»¶åæ˜¯å¦æ­£ç¡®ã€‚")
    exit()

model.eval()

class_names = {0: "æ— å®³", 1: "åˆ»æ¿å°è±¡ä¸åè§", 2: "è‰²æƒ…ä¸æ€§æš—ç¤º", 3: "ä¾®è¾±ä¸æ”»å‡»", 4: "è‡ªå˜²ä¸æ¶ˆæ"}
class_labels = list(class_names.values())

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


# --- Part 2: æ ¸å¿ƒé¢„æµ‹å‡½æ•° ---
def predict(image, text, text_description, meme_description):
    if image is None or not text:
        return {label: 0 for label in class_labels}

    # å¦‚æœæè¿°ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ä¸»æ–‡æœ¬
    if not text_description:
        text_description = text
    if not meme_description:
        meme_description = text

    if model_to_load == "clip":
        inputs = processor(text=text, images=image, return_tensors="pt", padding="max_length",
                           max_length=config.pad_size, truncation=True)
        td_inputs = processor(text=text_description, return_tensors="pt", padding="max_length",
                              max_length=config.pad_size, truncation=True)
        md_inputs = processor(text=meme_description, return_tensors="pt", padding="max_length",
                              max_length=config.pad_size, truncation=True)
        model_inputs = {"input_ids": inputs["input_ids"], "attention_mask": inputs["attention_mask"],
                        "image_tensor": inputs["pixel_values"], "text_discription_input_ids": td_inputs["input_ids"],
                        "text_discription_attention_mask": td_inputs["attention_mask"],
                        "meme_discription_input_ids": md_inputs["input_ids"],
                        "meme_discription_attention_mask": md_inputs["attention_mask"], }
    else:
        image_inputs = image_extractor(image, return_tensors='pt')
        text_inputs = tokenizer(text, max_length=config.pad_size, padding="max_length", truncation=True,
                                return_tensors="pt")
        td_inputs = tokenizer(text_description, max_length=config.pad_size, padding="max_length", truncation=True,
                              return_tensors="pt")
        md_inputs = tokenizer(meme_description, max_length=config.pad_size, padding="max_length", truncation=True,
                              return_tensors="pt")
        model_inputs = {"input_ids": text_inputs["input_ids"], "attention_mask": text_inputs["attention_mask"],
                        "image_tensor": image_inputs["pixel_values"],
                        "text_discription_input_ids": td_inputs["input_ids"],
                        "text_discription_attention_mask": td_inputs["attention_mask"],
                        "meme_discription_input_ids": md_inputs["input_ids"],
                        "meme_discription_attention_mask": md_inputs["attention_mask"], }

    for key, value in model_inputs.items():
        model_inputs[key] = value.to(config.device)

    with torch.no_grad():
        logit = model(**model_inputs).cpu()

    probabilities = torch.softmax(logit, dim=1).squeeze().numpy()
    return {class_labels[i]: float(probabilities[i]) for i in range(len(class_labels))}


# --- Part 3: æ‰¹é‡éªŒè¯ä¸ç»˜å›¾å‡½æ•° ---
def evaluate_on_demo_data():
    print("å¼€å§‹åœ¨demoæ•°æ®é›†ä¸Šè¿›è¡Œæ‰¹é‡éªŒè¯...")
    try:
        with open('demo_data.json', 'r', encoding='utf-8') as f:
            demo_data = json.load(f)
    except FileNotFoundError:
        return pd.DataFrame(), "é”™è¯¯: demo_data.json æœªæ‰¾åˆ°ã€‚", None, None, None

    results_list, true_labels_str, pred_labels_str = [], [], []
    for i, item in enumerate(demo_data):
        image_path = os.path.join(config.meme_path, item['new_path'])
        try:
            image = Image.open(image_path).convert('RGB')
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å›¾ç‰‡ {image_path}, è·³è¿‡ã€‚")
            continue

        # --- MODIFIED: ä¿®å¤ UnboundLocalError ---
        text = item['text']
        text_desc = item.get('text_discription', text)
        meme_desc = item.get('meme_discription', text)
        # --- ä¿®æ”¹ç»“æŸ ---

        prediction_scores = predict(image, text, text_desc, meme_desc)
        predicted_label = max(prediction_scores, key=prediction_scores.get)
        true_label = class_names.get(item['type'], "æœªçŸ¥")
        true_labels_str.append(true_label)
        pred_labels_str.append(predicted_label)
        results_list.append(
            {"å›¾ç‰‡": item['new_path'], "æ–‡æœ¬": text, "çœŸå®ç±»åˆ«": true_label, "é¢„æµ‹ç±»åˆ«": predicted_label,
             "æ˜¯å¦æ­£ç¡®": "âœ”ï¸" if true_label == predicted_label else "âŒ"})
        yield pd.DataFrame(results_list), f"å¤„ç†ä¸­... {i + 1}/{len(demo_data)}", None, None, None

    if not results_list:
        return pd.DataFrame(), "é”™è¯¯ï¼šæ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ•°æ®ã€‚", None, None, None

    report = classification_report(true_labels_str, pred_labels_str, labels=class_labels, output_dict=True,
                                   zero_division=0)
    summary_text = f"âœ… æ‰¹é‡éªŒè¯å®Œæˆï¼\n\n\næ€»è§ˆ:\n\n- æ€»å‡†ç¡®ç‡ : {report['accuracy']:.2%}\n- å®å¹³å‡F1åˆ†æ•°: {report['macro avg']['f1-score']:.4f}\n\n\nå›¾è¡¨å±•ç¤ºäº†æ›´è¯¦ç»†çš„æ€§èƒ½åˆ†æã€‚"

    cm = confusion_matrix(true_labels_str, pred_labels_str, labels=class_labels)
    fig_cm, ax_cm = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=class_labels, yticklabels=class_labels, ax=ax_cm)
    ax_cm.set_xlabel('é¢„æµ‹ç±»åˆ«');
    ax_cm.set_ylabel('çœŸå®ç±»åˆ«');
    ax_cm.set_title('æ··æ·†çŸ©é˜µ')
    plt.setp(ax_cm.get_xticklabels(), rotation=30, ha="right", rotation_mode="anchor")
    plt.setp(ax_cm.get_yticklabels(), rotation=0)
    fig_cm.tight_layout()

    report_df = pd.DataFrame(report).transpose()
    metrics_df = report_df.loc[class_labels, ['precision', 'recall', 'f1-score']]
    fig_metrics, ax_metrics = plt.subplots(figsize=(12, 7))
    metrics_df.plot(kind='bar', ax=ax_metrics, colormap='viridis')
    ax_metrics.set_title('å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡');
    ax_metrics.set_ylabel('åˆ†æ•°');
    ax_metrics.set_xlabel('ç±»åˆ«')
    plt.setp(ax_metrics.get_xticklabels(), rotation=30, ha="right", rotation_mode="anchor")
    ax_metrics.grid(axis='y', linestyle='--', alpha=0.7)
    fig_metrics.tight_layout()

    class_counts = pd.Series(true_labels_str).value_counts().reindex(class_labels, fill_value=0)
    fig_dist, ax_dist = plt.subplots(figsize=(8, 8))
    ax_dist.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=90,
                colors=sns.color_palette("pastel"))
    ax_dist.axis('equal');
    ax_dist.set_title('Demoæ•°æ®é›†ä¸­å„ç±»åˆ«åˆ†å¸ƒ')
    fig_dist.tight_layout()

    print("æ‰¹é‡éªŒè¯å®Œæˆï¼")
    yield pd.DataFrame(results_list), summary_text, fig_cm, fig_metrics, fig_dist


# --- Part 4: Gradio Web UI ç•Œé¢æ„å»º ---
with gr.Blocks(theme=gr.themes.Soft(), title="UGCå¤šæ¨¡æ€æ™ºèƒ½å®¡æ ¸ç³»ç»Ÿ") as demo:
    gr.Markdown(
        """ # ğŸ“ **UGCå¤šæ¨¡æ€æ™ºèƒ½å®¡æ ¸ç³»ç»Ÿ (æ·±åº¦èåˆæ”¹è¿›ç‰ˆ)**\næœ¬ç³»ç»ŸåŸºäºæ”¹è¿›çš„ **MHKE** æ¨¡å‹ã€‚\n**æ ¸å¿ƒåˆ›æ–°**: å¼•å…¥äº†åŸºäº **åŒå‘äº¤å‰æ³¨æ„åŠ›** çš„æ·±åº¦èåˆæ¨¡å—ï¼Œå®ç°äº†å¯¹æ–‡æœ¬ä¸å›¾åƒå†…å®¹çš„ä¸¥æ ¼è”åˆè¡¨å¾å­¦ä¹ ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†åŸå§‹æ¨¡å‹æµ…å±‚èåˆçš„ä¸è¶³ã€‚ """)
    with gr.Tabs():
        with gr.TabItem("äº¤äº’å¼é¢„æµ‹"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### 1. è¾“å…¥æ•°æ®")
                    input_image = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡")
                    input_text = gr.Textbox(lines=2, label="å›¾ç‰‡ä¸­çš„æ–‡æœ¬")
                    input_text_desc = gr.Textbox(lines=2, label="æ–‡æœ¬æè¿°", placeholder="è‹¥ç•™ç©ºï¼Œå°†ä½¿ç”¨æ–‡æœ¬ä»£æ›¿")
                    input_meme_desc = gr.Textbox(lines=2, label="å›¾åƒæè¿°", placeholder="è‹¥ç•™ç©ºï¼Œå°†ä½¿ç”¨æ–‡æœ¬ä»£æ›¿")
                    predict_btn = gr.Button("ğŸš€ å¼€å§‹é¢„æµ‹", variant="primary", scale=2)
                with gr.Column(scale=1):
                    gr.Markdown("### 2. é¢„æµ‹ç»“æœ")
                    output_label = gr.Label(label="åˆ†ç±»æ¦‚ç‡", num_top_classes=5)

            gr.Examples(
                examples=[
                    [os.path.join(config.meme_path, "10768.jpg"), "è¦è¯´å‚»é€¼è°èƒ½æ¯”è¿‡ä½ ", "è¿™æ˜¯ä¸€å¥å¸¦æœ‰ä¾®è¾±æ€§è´¨çš„è¯è¯­...",
                     "è¿™æ˜¯ä¸€å¹…å¡é€šåŒ–çš„å›¾ç”»..."],
                    [os.path.join(config.meme_path, "79.jpg"), "ä½ ä»Šå¤©å¿˜è®°æ‰‹å†²äº†ã€‚é—­å˜´æˆ‘æˆ’äº†ã€‚",
                     "è¿™å¥è¯å¯èƒ½æ˜¯ä¸¤ä¸ªäººå¯¹è¯...", "ä¸€å¼ æ¼«ç”»..."],
                    [os.path.join(config.meme_path, "11541.jpg"), "æƒ³éé›¢é–‹ï¼ æ˜¯å› ç‚ºé‚£äº›å§¿æ…‹é‚£äº›æ—ç™½é‚£äº›å‚·å®³...",
                     "è¿™æ®µæ–‡æœ¬è¡¨è¾¾äº†ä½œè€…å†…å¿ƒçš„ç—›è‹¦å’Œæ— åŠ©...", "ä¸€ä½æˆ´ç€è€³æœºã€å¿ƒç¢è¡¨æƒ…çš„å¡é€šèœœèœ‚..."],
                ],
                inputs=[input_image, input_text, input_text_desc, input_meme_desc],
                label="ç¤ºä¾‹æ•°æ® (ç‚¹å‡»è‡ªåŠ¨å¡«å……)"
            )
        with gr.TabItem("æ‰¹é‡æ•°æ®éªŒè¯"):
            gr.Markdown("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ï¼Œç³»ç»Ÿå°†è¯»å– `demo_data.json` ä¸­çš„å…¨éƒ¨æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å±•ç¤ºè¯¦ç»†ç»“æœä¸æ€§èƒ½è¯„ä¼°å›¾è¡¨ã€‚")
            eval_btn = gr.Button("ğŸ” è¿è¡Œå…¨ä½“éªŒè¯é›†", variant="primary")
            with gr.Row():
                with gr.Column(scale=1):
                    eval_status = gr.Textbox(label="ğŸ“Š æ€§èƒ½æ‘˜è¦", lines=6)
                    class_distribution_plot = gr.Plot(label="ç±»åˆ«åˆ†å¸ƒ")
                with gr.Column(scale=1):
                    confusion_matrix_plot = gr.Plot(label="æ··æ·†çŸ©é˜µ")
                    metrics_bar_chart_plot = gr.Plot(label="å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡")
            results_dataframe = gr.DataFrame(headers=["å›¾ç‰‡", "æ–‡æœ¬", "çœŸå®ç±»åˆ«", "é¢„æµ‹ç±»åˆ«", "æ˜¯å¦æ­£ç¡®"],
                                             label="è¯¦ç»†é¢„æµ‹ç»“æœ", wrap=True)

    predict_btn.click(fn=predict, inputs=[input_image, input_text, input_text_desc, input_meme_desc],
                      outputs=output_label)
    eval_btn.click(fn=evaluate_on_demo_data,
                   outputs=[results_dataframe, eval_status, confusion_matrix_plot, metrics_bar_chart_plot,
                            class_distribution_plot])

print("Gradioç•Œé¢å‡†å¤‡å°±ç»ªï¼Œæ­£åœ¨å¯åŠ¨...")
demo.launch(share=True)